{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#neural-networks-coursework","title":"Neural Networks Coursework","text":"<p>Author: Lucca Hiratsuca Costa</p> <p>Project Group Members:</p> <ul> <li> <p>Felipe Maluli</p> </li> <li> <p>Lucca Hiratsuca Costa</p> </li> <li> <p>Thomas Chiari Ciocchetti de Souza</p> </li> </ul>"},{"location":"#deliverables","title":"Deliverables","text":"<p>Each dealiverable exercise was made individually</p> <ul> <li> <p> Data Task - 05/09/2025 </p> </li> <li> <p> Perceptron Task - 14/09/2025</p> </li> <li> <p> MLP Task - 21/08/2025</p> </li> <li> <p> VAE Task - 26/09/2025</p> </li> <li> <p> Classification Project - 05/10/2025</p> </li> <li> <p> Regression Project - 26/10/2025</p> </li> <li> <p> Generative Project - 16/11/2025</p> </li> </ul>"},{"location":"#references","title":"References","text":"<p>Material for MkDocs</p>"},{"location":"classification-project/main/","title":"Classification Project","text":""},{"location":"classification-project/main/#classification-project","title":"Classification Project","text":"<p>Documentation &amp; Report: https://thomaschiari.github.io/deep-learning-coursework/classification-project/classification-project/</p>"},{"location":"exercises/data/main/","title":"Data Task","text":""},{"location":"exercises/data/main/#data","title":"Data","text":"<p>This report summarizes the work done in Exercises 1, 2, and 3 of the Data Preparation and Analysis for Neural Networks activity. All generated figures are stored in the <code>./images</code> folder. Each section below explains what we did, shows the figures, and provides brief interpretations.</p>"},{"location":"exercises/data/main/#exercise-1-class-separability-in-2d","title":"Exercise 1 \u2014 Class Separability in 2D","text":""},{"location":"exercises/data/main/#goal","title":"Goal","text":"<p>We created a synthetic 2D dataset with four Gaussian-distributed classes. The purpose was to investigate class separability and compare linear versus non-linear decision boundaries.</p>"},{"location":"exercises/data/main/#data-distribution","title":"Data distribution","text":"<p>The scatter plot shows how the four Gaussian classes overlap and spread in the 2D feature space. </p>"},{"location":"exercises/data/main/#decision-boundaries","title":"Decision boundaries","text":"<p>We trained multiple classifiers and visualized their boundaries:</p> <ul> <li> <p>Logistic Regression \u2014 A linear classifier. The boundary is straight lines, which may not fully separate overlapping regions. </p> </li> <li> <p>LDA \u2014 Also linear, assumes Gaussian distributions with shared covariance. Works well if assumptions hold. </p> </li> <li> <p>Linear SVM \u2014 Maximizes margins but restricted to linear separation. </p> </li> <li> <p>RBF-SVM \u2014 Uses a radial basis kernel to capture non-linear regions, fitting more complex shapes. </p> </li> <li> <p>MLP (2 hidden layers) \u2014 A neural network that can approximate complex non-linear decision regions. </p> </li> </ul>"},{"location":"exercises/data/main/#key-observations","title":"Key observations","text":"<ul> <li>Linear models (LogReg, LDA, Linear SVM) produce straight boundaries, which may leave misclassified regions.  </li> <li>Non-linear models (RBF-SVM, MLP) adapt to the curved boundaries and usually achieve higher accuracy.  </li> <li>This exercise demonstrates why neural networks are powerful for non-linear class separation.</li> </ul>"},{"location":"exercises/data/main/#exercise-2-non-linearity-in-higher-dimensions","title":"Exercise 2 \u2014 Non-linearity in Higher Dimensions","text":""},{"location":"exercises/data/main/#goal_1","title":"Goal","text":"<p>We generated two classes in 6 dimensions using different means and covariance structures. We reduced the data to 2D with PCA to visualize separability, then compared linear and non-linear models in both 6D and PCA-2D spaces.</p>"},{"location":"exercises/data/main/#pca-projection","title":"PCA projection","text":"<p>The PCA scatter plot shows the two classes after dimensionality reduction. While there is some separation, the overlap indicates the problem is not linearly separable in 2D. </p>"},{"location":"exercises/data/main/#decision-boundary-non-linear-model","title":"Decision boundary (non-linear model)","text":"<p>We trained an RBF-SVM on the PCA-reduced space. The figure shows how the kernel-based classifier creates non-linear boundaries. </p>"},{"location":"exercises/data/main/#key-observations_1","title":"Key observations","text":"<ul> <li>In the full 6D space, non-linear models achieve higher performance than linear ones, as expected.  </li> <li>PCA helps visualize structure but loses information \u2014 some linear separability is hidden in higher dimensions.  </li> <li>Neural networks and kernel methods (like RBF-SVM) are better suited for this type of complex data.</li> </ul>"},{"location":"exercises/data/main/#exercise-3-titanic-dataset-real-world-preprocessing-classification","title":"Exercise 3 \u2014 Titanic Dataset (Real-World Preprocessing &amp; Classification)","text":""},{"location":"exercises/data/main/#goal_2","title":"Goal","text":"<p>We prepared and modeled the Titanic dataset (or a synthetic version if the real one is not present). The objective was to practice data preprocessing and classification pipeline design.</p>"},{"location":"exercises/data/main/#preprocessing-steps","title":"Preprocessing steps","text":"<ul> <li>Missing values: filled numeric features with median, categorical with the most frequent value.  </li> <li>Encoding: one-hot encoding for categorical variables.  </li> <li>Scaling: standardized numeric features.  </li> <li>Target: <code>Survived</code> column (binary classification).</li> </ul>"},{"location":"exercises/data/main/#confusion-matrix","title":"Confusion Matrix","text":"<p>The confusion matrix for the best-performing model shows correct vs. incorrect predictions for survivors and non-survivors. </p>"},{"location":"exercises/data/main/#roc-curve","title":"ROC Curve","text":"<p>The ROC curve illustrates the trade-off between true positives and false positives, with the AUC score summarizing classifier performance. </p>"},{"location":"exercises/data/main/#key-observations_2","title":"Key observations","text":"<ul> <li>Non-linear models (Random Forest, MLP, RBF-SVM) typically outperform simple Logistic Regression on this dataset.  </li> <li>The ROC curve and AUC are particularly useful because class imbalance exists in Titanic (more passengers did not survive).  </li> <li>Feature engineering (family size, title extraction) could further improve results beyond the baseline preprocessing.</li> </ul>"},{"location":"exercises/data/main/#final-notes","title":"Final Notes","text":"<ul> <li>Each exercise progresses from synthetic low-dimensional data (clear visualization of linear vs. non-linear boundaries), to high-dimensional synthetic data (importance of kernels and neural nets), and finally to a real-world dataset (end-to-end preprocessing and modeling).  </li> <li>The generated figures provide both intuition and evidence for the performance of linear vs. non-linear models.  </li> <li>All figures are automatically exported into the <code>./images</code> folder by the notebook.</li> </ul>"},{"location":"exercises/mlp/main/","title":"MLP Task","text":""},{"location":"exercises/mlp/main/#mlp-implementation","title":"MLP Implementation","text":""},{"location":"exercises/mlp/main/#1-introduction","title":"1. Introduction","text":"<p>This report implements and analyzes four exercises on Multi-Layer Perceptrons (MLPs) using only NumPy (for the \u201cfrom-scratch\u201d parts). We strictly follow the assignment constraints:</p> <ul> <li>Hidden activation: tanh (for all scratch MLPs).</li> <li>Exercise 1 output activation: tanh (as required by the prompt).</li> <li>Binary classifier (Ex. 2) output: sigmoid with binary cross-entropy (BCE).</li> <li>Multiclass (Ex. 3\u20134) output: softmax with cross-entropy (CE).</li> <li>Learning rate for training: \\(\\eta = 0.3\\) (as requested).</li> <li>Reproducible synthetic datasets (2D) to visualize decision boundaries.</li> </ul> <p>Rubric alignment: - Clear math derivations (Ex. 1). - Clean, commented code (Ex. 2\u20134). - Required figures: learning curves (loss/accuracy), decision boundaries, confusion matrices, and early-stopping trend. - Concise discussions with quantitative outcomes and error analysis.</p>"},{"location":"exercises/mlp/main/#exercise-1-manual-calculation-of-an-mlp-tanh-eta03","title":"Exercise 1 \u2014 Manual Calculation of an MLP (tanh, \\(\\eta=0.3\\))","text":""},{"location":"exercises/mlp/main/#exercise-1-manual-calculation-of-a-1-hidden-layer-mlp-tanh-eta03","title":"Exercise 1 \u2014 Manual Calculation of a 1-Hidden-Layer MLP (tanh, \\(\\eta=0.3\\))","text":"<p>We manually compute a forward and backward pass (with parameter updates) for a tiny MLP: - Hidden activation: tanh - Output activation: tanh - Loss: Mean Squared Error (MSE) with a single sample (\\(N=1\\)) - Learning rate: \\(\\eta = 0.3\\)</p>"},{"location":"exercises/mlp/main/#setup-given","title":"Setup (given)","text":"<ul> <li> <p>Input \\(\\mathbf{x} = [\\,0.5,\\; -0.2\\,]\\)</p> </li> <li> <p>Target \\(y = 1.0\\)</p> </li> <li> <p>Parameters \\(\\displaystyle   \\mathbf{W}^{(1)} =   \\begin{bmatrix}   0.3 &amp; -0.1\\\\   0.2 &amp; \\;\\;0.4   \\end{bmatrix},\\quad   \\mathbf{b}^{(1)} = [\\,0.1,\\; -0.2\\,]\\)</p> </li> </ul> <p>\\(\\displaystyle   \\mathbf{W}^{(2)} =   \\begin{bmatrix}   0.5\\\\  -0.3   \\end{bmatrix},\\quad   b^{(2)} = 0.2\\)</p> <ul> <li> <p>Activation and its derivative \\(\\displaystyle \\tanh(u) \\quad\\text{and}\\quad \\frac{d}{du}\\tanh(u) = 1-\\tanh^2(u)\\)</p> </li> <li> <p>MSE loss (single example) \\(\\displaystyle L = (y - \\hat y)^2\\)</p> </li> </ul>"},{"location":"exercises/mlp/main/#1-forward-pass","title":"1) Forward Pass","text":""},{"location":"exercises/mlp/main/#a-hidden-pre-activations","title":"(a) Hidden pre-activations","text":"<p>Formula used: For a fully connected layer, \\(\\displaystyle \\mathbf{z}^{(1)} = \\mathbf{x}\\,\\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}.\\)</p> <p>Plugging the numbers:</p> \\[ \\begin{aligned} z^{(1)}_1 &amp;= 0.5(0.3) + (-0.2)(0.2) + 0.1 = 0.15 - 0.04 + 0.10 = \\boxed{0.210000},\\\\[4pt] z^{(1)}_2 &amp;= 0.5(-0.1) + (-0.2)(0.4) + (-0.2) = -0.05 - 0.08 - 0.20 = \\boxed{-0.330000}. \\end{aligned} \\] <p>So \\(\\displaystyle \\boxed{\\mathbf{z}^{(1)} = [\\,0.210000,\\; -0.330000\\,]}.\\)</p>"},{"location":"exercises/mlp/main/#b-hidden-activations","title":"(b) Hidden activations","text":"<p>Formula used: \\(\\displaystyle \\mathbf{h}^{(1)} = \\tanh(\\mathbf{z}^{(1)}).\\)</p> <p>Plugging the numbers:</p> \\[ \\boxed{\\mathbf{h}^{(1)} = [\\,\\tanh(0.210000),\\; \\tanh(-0.330000)\\,] = [\\,0.20696650,\\; -0.31852078\\,]}. \\]"},{"location":"exercises/mlp/main/#c-output-pre-activation","title":"(c) Output pre-activation","text":"<p>Formula used: \\(\\displaystyle u^{(2)} = \\mathbf{h}^{(1)}\\mathbf{W}^{(2)} + b^{(2)}.\\)</p> <p>Plugging the numbers:</p> \\[ u^{(2)} = (0.20696650)(0.5) + (-0.31852078)(-0.3) + 0.2 = 0.10348325 + 0.09555623 + 0.20000000 = \\boxed{0.39903948}. \\]"},{"location":"exercises/mlp/main/#d-final-output-tanh","title":"(d) Final output (tanh)","text":"<p>Formula used: \\(\\displaystyle \\hat y = \\tanh\\!\\big(u^{(2)}\\big).\\)</p> <p>Plugging the number:</p> \\[ \\boxed{\\hat y = \\tanh(0.39903948) = 0.37912681}. \\]"},{"location":"exercises/mlp/main/#2-loss-mse","title":"2) Loss (MSE)","text":"<p>Formula used (single example): \\(\\displaystyle L = (y - \\hat y)^2.\\)</p> <p>Plugging the numbers:</p> \\[ \\boxed{L = (1.0 - 0.37912681)^2 = 0.38548352}. \\]"},{"location":"exercises/mlp/main/#3-backward-pass-gradients","title":"3) Backward Pass (Gradients)","text":"<p>We apply the chain rule from the output to the input parameters.</p>"},{"location":"exercises/mlp/main/#a-output-node","title":"(a) Output node","text":"<p>Formulas used:</p> <ol> <li>\\(\\displaystyle \\frac{\\partial L}{\\partial \\hat y} = 2(\\hat y - y)\\) </li> <li>\\(\\displaystyle \\frac{\\partial \\hat y}{\\partial u^{(2)}} = 1 - \\tanh^2(u^{(2)}) = 1 - \\hat y^2\\) </li> <li>\\(\\displaystyle \\frac{\\partial L}{\\partial u^{(2)}} = \\frac{\\partial L}{\\partial \\hat y}\\cdot \\frac{\\partial \\hat y}{\\partial u^{(2)}}\\)</li> </ol> <p>Plugging the numbers:</p> \\[ \\frac{\\partial L}{\\partial \\hat y} = 2(0.37912681 - 1.0) = \\boxed{-1.24174638} \\] \\[ \\frac{\\partial \\hat y}{\\partial u^{(2)}} = 1 - (0.37912681)^2 = \\boxed{0.85626286} \\] \\[ \\frac{\\partial L}{\\partial u^{(2)}} = (-1.24174638)(0.85626286) = \\boxed{-1.06326131} \\]"},{"location":"exercises/mlp/main/#b-output-layer-parameters-mathbfw2-b2","title":"(b) Output layer parameters \\((\\mathbf{W}^{(2)}, b^{(2)})\\)","text":"<p>Formulas used (linear layer): 1. \\(\\displaystyle \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} = \\mathbf{h}^{(1)\\top}\\frac{\\partial L}{\\partial u^{(2)}}\\) 2. \\(\\displaystyle \\frac{\\partial L}{\\partial b^{(2)}} = \\frac{\\partial L}{\\partial u^{(2)}}\\)</p> <p>Plugging the numbers:</p> \\[ \\boxed{ \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} = \\begin{bmatrix} 0.20696650\\\\[2pt] -0.31852078 \\end{bmatrix} (-1.06326131) = \\begin{bmatrix} -0.22005947\\\\[2pt] \\;\\;0.33867082 \\end{bmatrix} } \\qquad \\boxed{\\frac{\\partial L}{\\partial b^{(2)}} = -1.06326131} \\]"},{"location":"exercises/mlp/main/#c-backprop-to-hidden","title":"(c) Backprop to hidden","text":"<p>Formulas used: 1. \\(\\displaystyle \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} = \\frac{\\partial L}{\\partial u^{(2)}}\\,\\mathbf{W}^{(2)\\top}\\) 2. \\(\\displaystyle \\frac{\\partial \\mathbf{h}^{(1)}}{\\partial \\mathbf{z}^{(1)}} = 1 - \\tanh^2(\\mathbf{z}^{(1)})\\) 3. Element-wise chain: \\(\\displaystyle \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} =    \\left(\\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}}\\right)    \\odot    \\left(\\frac{\\partial \\mathbf{h}^{(1)}}{\\partial \\mathbf{z}^{(1)}}\\right)\\)</p> <p>Plugging the numbers:</p> \\[ \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} = (-1.06326131)\\,[\\,0.5,\\;-0.3\\,] = \\boxed{[\\, -0.53163065,\\;\\;0.31897839\\,]} \\] \\[ \\frac{\\partial \\mathbf{h}^{(1)}}{\\partial \\mathbf{z}^{(1)}} = 1 - [\\,\\tanh(0.210000)^2,\\;\\tanh(-0.330000)^2\\,] = \\boxed{[\\,0.95716487,\\;0.89854451\\,]} \\] \\[ \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} = [\\, -0.53163065,\\;\\;0.31897839\\,] \\odot [\\,0.95716487,\\;0.89854451\\,] = \\boxed{[\\, -0.50885819,\\;\\;0.28661628\\,]} \\]"},{"location":"exercises/mlp/main/#d-hidden-layer-parameters-mathbfw1-mathbfb1","title":"(d) Hidden layer parameters \\((\\mathbf{W}^{(1)}, \\mathbf{b}^{(1)})\\)","text":"<p>Formulas used (linear layer): 1. \\(\\displaystyle \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} = \\mathbf{x}^\\top \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}}\\) 2. \\(\\displaystyle \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}}\\) (sum over batch; here \\(N=1\\))</p> <p>Plugging the numbers:</p> \\[ \\boxed{ \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} = \\begin{bmatrix} 0.5\\\\[2pt] -0.2 \\end{bmatrix} \\begin{bmatrix} -0.50885819 &amp; \\;\\;0.28661628 \\end{bmatrix} = \\begin{bmatrix} -0.25442909 &amp; \\;\\;0.14330814\\\\[2pt] \\;\\;0.10177164 &amp; -0.05732326 \\end{bmatrix} } \\] \\[ \\boxed{ \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} = [\\, -0.50885819,\\;\\; 0.28661628\\,] } \\]"},{"location":"exercises/mlp/main/#4-parameter-update-gradient-descent-eta03","title":"4) Parameter Update (Gradient Descent, \\(\\eta=0.3\\))","text":"<p>Update rule used (all parameters): \\(\\displaystyle \\theta \\leftarrow \\theta - \\eta \\,\\frac{\\partial L}{\\partial \\theta}.\\)</p>"},{"location":"exercises/mlp/main/#a-output-layer","title":"(a) Output layer","text":"<p>Plugging the numbers:</p> \\[ \\mathbf{W}^{(2)}_{\\text{new}} = \\begin{bmatrix} 0.5\\\\[2pt] -0.3 \\end{bmatrix} - 0.3 \\begin{bmatrix} -0.22005947\\\\[2pt] \\;\\;0.33867082 \\end{bmatrix} = \\boxed{ \\begin{bmatrix} 0.56601784\\\\[2pt] -0.40160125 \\end{bmatrix}} \\] \\[ b^{(2)}_{\\text{new}} = 0.2 - 0.3(-1.06326131) = \\boxed{0.51897839} \\]"},{"location":"exercises/mlp/main/#b-hidden-layer","title":"(b) Hidden layer","text":"<p>Plugging the numbers:</p> \\[ \\mathbf{W}^{(1)}_{\\text{new}} = \\begin{bmatrix} 0.3 &amp; -0.1\\\\[2pt] 0.2 &amp; \\;\\;0.4 \\end{bmatrix} - 0.3 \\begin{bmatrix} -0.25442909 &amp; \\;\\;0.14330814\\\\[2pt] \\;\\;0.10177164 &amp; -0.05732326 \\end{bmatrix} = \\boxed{ \\begin{bmatrix} 0.37632873 &amp; -0.14299244\\\\[2pt] 0.16946851 &amp; \\;\\;0.41719698 \\end{bmatrix}} \\] \\[ \\mathbf{b}^{(1)}_{\\text{new}} = [\\,0.1,\\; -0.2\\,] - 0.3[\\, -0.50885819,\\; 0.28661628\\,] = \\boxed{[\\,0.25265746,\\; -0.28598488\\,]} \\]"},{"location":"exercises/mlp/main/#5-flat-summary-no-matrices","title":"5) Flat Summary (no matrices)","text":"<ul> <li>\\(W^{(2)} = [\\,\\mathbf{0.56601784},\\; \\mathbf{-0.40160125}\\,]\\) </li> <li>\\(b^{(2)} = \\mathbf{0.51897839}\\) </li> <li>\\(W^{(1)}_{11}=\\mathbf{0.37632873}\\), \\(W^{(1)}_{12}=\\mathbf{-0.14299244}\\), \\(W^{(1)}_{21}=\\mathbf{0.16946851}\\), \\(W^{(1)}_{22}=\\mathbf{0.41719698}\\) </li> <li>\\(b^{(1)} = [\\,\\mathbf{0.25265746},\\; \\mathbf{-0.28598488}\\,]\\)</li> </ul>"},{"location":"exercises/mlp/main/#optional-reproducibility-minimal-numpy-script","title":"(Optional) Reproducibility \u2014 Minimal NumPy script","text":"<p>Use tanh output + MSE + \\(\\eta=0.3\\) to get the same numbers above.</p> <pre><code>import numpy as np\n\n# Given\nx = np.array([[0.5, -0.2]])\nt = np.array([[1.0]])\n\nW1 = np.array([[0.3, -0.1],\n               [0.2,  0.4]])\nb1 = np.array([[0.1, -0.2]])\n\nW2 = np.array([[ 0.5],\n               [-0.3]])\nb2 = np.array([[0.2]])\n\neta = 0.3\n\n# Activations\ntanh  = np.tanh\ndtanh = lambda z: 1 - np.tanh(z)**2\n\n# Forward (tanh output)\nz1 = x @ W1 + b1\nh1 = tanh(z1)\nu2 = h1 @ W2 + b2\ny  = tanh(u2)\nL  = (t - y)**2    # MSE, N=1\n\n# Backward\ndL_dy  = 2*(y - t)\ndy_du2 = dtanh(u2)\ndL_du2 = dL_dy * dy_du2\n\ndW2 = h1.T @ dL_du2\ndb2 = dL_du2\n\ndh1 = dL_du2 @ W2.T\ndz1 = dh1 * dtanh(z1)\ndW1 = x.T @ dz1\ndb1 = dz1\n\n# Update\nW2_new = W2 - eta * dW2\nb2_new = b2 - eta * db2\nW1_new = W1 - eta * dW1\nb1_new = b1 - eta * db1\n\nprint(\"W2_new:\\n\", W2_new)\nprint(\"b2_new:\\n\", b2_new)\nprint(\"W1_new:\\n\", W1_new)\nprint(\"b1_new:\\n\", b1_new)\n</code></pre>"},{"location":"exercises/mlp/main/#exercise-2-binary-classification-with-synthetic-data-scratch-mlp","title":"Exercise 2 \u2014 Binary Classification with Synthetic Data (Scratch MLP)","text":"<p>Goal. Train a from-scratch MLP on a 2-class 2D dataset; report loss/accuracy curves, decision boundary, and confusion matrix.</p> <p>Data. Two overlapping blobs/rings in \\(\\mathbb{R}^2\\) (1,000 samples; 75% train / 25% val).</p> <p>Model. - Architecture: \\(2 \\rightarrow H \\rightarrow 1\\) with \\(H=8\\). - Hidden activation: tanh; Output: sigmoid. - Loss: BCE; regularization: small L2 (\\(10^{-4}\\)). - Training: full-batch GD, \\(\\eta=0.3\\), 250 epochs.</p> <p>Learning curves. </p> <p>Decision boundary (validation). </p> <p>Confusion matrix (validation). </p> <p>Outcomes &amp; discussion. - BCE loss decreases smoothly on both train/val (no divergence), indicating stable optimization with \\(\\eta=0.3\\). - The decision boundary is nonlinear (as expected with tanh), separating most points with a gentle transition band. - Confusion matrix counts (example run): \\(\\begin{bmatrix} 86 &amp; 33 \\\\ 46 &amp; 85 \\end{bmatrix}\\) - Potential improvements: larger \\(H\\), additional layer, tuned regularization, or data standardization.</p>"},{"location":"exercises/mlp/main/#exercise-3-multi-class-classification-with-reusable-mlp","title":"Exercise 3 \u2014 Multi-Class Classification with Reusable MLP","text":"<p>Goal. Build a reusable MLP class (arbitrary hidden layers), train on a 3-class synthetic dataset, and report loss/accuracy, decision boundary, and confusion matrix.</p> <p>Data. Three clusters (900 samples total), 75%/25% split.</p> <p>Model. - Architecture: \\(2 \\rightarrow 16 \\rightarrow 16 \\rightarrow 3\\). - Hidden activation: tanh; Output: softmax with cross-entropy. - Training: full-batch GD, \\(\\eta=0.3\\), 250 epochs, L2 \\(10^{-4}\\).</p> <p>Loss curve (train vs val). </p> <p>Accuracy curve (train vs val). </p> <p>Decision boundary (validation). </p> <p>Confusion matrix (validation). </p> <p>Outcomes &amp; discussion. - CE loss decreases; val accuracy reaches ~0.72. - Decision regions are curved with a small ambiguous pocket at class intersections. - Example CM: rows show most confusion on class 2; adding capacity or light dropout could help.</p>"},{"location":"exercises/mlp/main/#exercise-4-deeper-mlp-regularization-early-stopping","title":"Exercise 4 \u2014 Deeper MLP + Regularization &amp; Early Stopping","text":"<p>Goal. Compare a shallow vs a deeper MLP on the same 3-class dataset. Use dropout and early stopping based on validation loss.</p> <p>Models. - A (shallow): \\(2 \\rightarrow 16 \\rightarrow 3\\), tanh. - B (deeper): \\(2 \\rightarrow 32 \\rightarrow 32 \\rightarrow 3\\), tanh, dropout \\(p=0.2\\). - Optimizer: full-batch GD, \\(\\eta=0.3\\); L2 \\(10^{-4}\\). - Early stopping: patience 25 (A) / 35 (B), restore best val loss.</p> <p>Early-stopping trend (validation loss). </p> <p>Decision boundary \u2014 A (shallow). </p> <p>Decision boundary \u2014 B (deeper+dropout). </p> <p>Outcomes &amp; discussion. - B atinge menor val loss (\u22480.62 vs \u22480.65) e fronteiras mais precisas na regi\u00e3o de interse\u00e7\u00e3o. - Dropout + early stopping reduzem overfitting mantendo expressividade.</p>"},{"location":"exercises/perceptron/main/","title":"Perceptron Task","text":""},{"location":"exercises/perceptron/main/#perceptron-implementation","title":"Perceptron Implementation","text":""},{"location":"exercises/perceptron/main/#1-introduction","title":"1. Introduction","text":"<p>This report documents the full implementation of the perceptron learning algorithm from scratch, following the assignment requirements. The objectives are:</p> <ul> <li>Generate synthetic datasets using multivariate Gaussian distributions.  </li> <li>Implement the perceptron update rule using only NumPy.  </li> <li>Train and evaluate the model on linearly separable and non-linearly separable data.  </li> <li>Visualize the data distribution, decision boundaries, misclassified points, and accuracy curves.  </li> <li>Compare expected outcomes with actual results and discuss limitations.  </li> </ul>"},{"location":"exercises/perceptron/main/#2-the-perceptron-model","title":"2. The Perceptron Model","text":"<p>The perceptron is a binary linear classifier. It learns a separating hyperplane of the form:</p> \\[ f(\\mathbf{x}) = \\text{sign}(\\mathbf{w}\\cdot \\mathbf{x} + b) \\] <p>where:</p> <ul> <li>\\(\\mathbf{w}\\) is the weight vector,  </li> <li>\\(b\\) is the bias,  </li> <li>\\(\\text{sign}(\\cdot)\\) determines the predicted class label.  </li> </ul>"},{"location":"exercises/perceptron/main/#update-rule","title":"Update Rule","text":"<p>If a sample \\((\\mathbf{x}, y)\\), with \\(y \\in \\{-1, +1\\}\\), is misclassified, the perceptron parameters are updated as:</p> \\[ \\mathbf w \\leftarrow \\mathbf w + \\eta \\, y \\, \\mathbf x,  \\qquad b \\leftarrow b + \\eta \\, y \\] <ul> <li>Learning rate: \\(\\eta = 0.01\\) </li> <li>Stopping criteria:  </li> <li>stop early if an epoch has zero mistakes,  </li> <li>or after a maximum of 100 epochs.  </li> </ul> <p>This iterative process is known as the online perceptron algorithm.</p>"},{"location":"exercises/perceptron/main/#3-exercise-1-linearly-separable-data","title":"3. Exercise 1: Linearly Separable Data","text":""},{"location":"exercises/perceptron/main/#assignment-description","title":"Assignment Description","text":"<p>We were asked to generate two Gaussian clusters that are linearly separable and train the perceptron to verify convergence.</p> <ul> <li>Class 0: mean = \\([1.5, 1.5]\\), covariance = \\(\\begin{bmatrix}0.5 &amp; 0 \\\\ 0 &amp; 0.5\\end{bmatrix}\\) </li> <li>Class 1: mean = \\([5, 5]\\), covariance = \\(\\begin{bmatrix}0.5 &amp; 0 \\\\ 0 &amp; 0.5\\end{bmatrix}\\) </li> </ul>"},{"location":"exercises/perceptron/main/#our-approach","title":"Our Approach","text":"<ul> <li>Generated 2000 samples (1000 per class).  </li> <li>Mapped labels from \\(\\{0,1\\}\\) to \\(\\{-1,+1\\}\\) for perceptron training.  </li> <li>Initialized weights randomly and trained with the perceptron update rule.  </li> <li>Stopped training when no errors were found in an epoch.  </li> </ul>"},{"location":"exercises/perceptron/main/#mathematical-expectation","title":"Mathematical Expectation","text":"<p>Since the two Gaussian clusters are well separated, the dataset is linearly separable. Thus, the perceptron must converge in finite steps, according to the Perceptron Convergence Theorem.</p>"},{"location":"exercises/perceptron/main/#results","title":"Results","text":"<p>Data distribution:</p> <p></p> <p>Decision boundary (converged after 34 epochs):</p> <p></p> <p>Accuracy progression:</p> <p></p>"},{"location":"exercises/perceptron/main/#analysis","title":"Analysis","text":"<ul> <li>Convergence achieved after 34 epochs.  </li> <li>Final accuracy = 100%.  </li> <li>The model found a separating hyperplane consistent with the Perceptron Convergence Theorem.  </li> </ul>"},{"location":"exercises/perceptron/main/#4-exercise-2-non-linearly-separable-data","title":"4. Exercise 2: Non-Linearly Separable Data","text":""},{"location":"exercises/perceptron/main/#assignment-description_1","title":"Assignment Description","text":"<p>We were asked to generate two Gaussian clusters that overlap so that the dataset is not linearly separable.</p> <ul> <li>Class 0: mean = \\([3, 3]\\), covariance = \\(\\begin{bmatrix}1.5 &amp; 0 \\\\ 0 &amp; 1.5\\end{bmatrix}\\) </li> <li>Class 1: mean = \\([5, 5]\\), covariance = \\(\\begin{bmatrix}1.5 &amp; 0 \\\\ 0 &amp; 1.5\\end{bmatrix}\\) </li> </ul>"},{"location":"exercises/perceptron/main/#our-approach_1","title":"Our Approach","text":"<ul> <li>Generated overlapping clusters (again 2000 samples).  </li> <li>Trained the perceptron with 5 different random initializations to test robustness.  </li> <li>Recorded accuracy progression and best run.  </li> </ul>"},{"location":"exercises/perceptron/main/#mathematical-expectation_1","title":"Mathematical Expectation","text":"<p>For non-linearly separable data, the perceptron cannot converge because no separating hyperplane exists. The model will continue to oscillate, and accuracy will remain close to random guessing (~50%).</p>"},{"location":"exercises/perceptron/main/#results_1","title":"Results","text":"<p>Data distribution:</p> <p></p> <p>Decision boundary (example run):</p> <p></p> <p>Best of 5 runs:</p> <p></p> <p>Accuracy progression (example run):</p> <p></p> <p>Accuracy across multiple runs:</p> <p></p>"},{"location":"exercises/perceptron/main/#analysis_1","title":"Analysis","text":"<ul> <li>Accuracy fluctuated between 50\u201352%, regardless of initialization.  </li> <li>Training never reached zero mistakes in an epoch.  </li> <li>Misclassified points remained even after 100 epochs.  </li> <li>Confirms theoretical limitation: perceptron cannot solve non-linearly separable problems.  </li> </ul>"},{"location":"exercises/perceptron/main/#5-conclusion","title":"5. Conclusion","text":"<ul> <li>Exercise 1: The perceptron converged perfectly, achieving 100% accuracy with linearly separable data.  </li> <li>Exercise 2: With overlapping classes, the perceptron failed to converge, stabilizing near 50% accuracy.  </li> </ul> <p>This experiment demonstrates the strength and limitation of the perceptron: - It guarantees convergence on linearly separable data. - It fails on non-linearly separable datasets due to the lack of a perfect separating hyperplane.  </p>"},{"location":"exercises/perceptron/main/#6-evaluation-criteria-alignment","title":"6. Evaluation Criteria Alignment","text":"<ul> <li>Correctness of implementation: perceptron built from scratch with NumPy only.  </li> <li>Exercise 1: demonstrated convergence as predicted by theory.  </li> <li>Exercise 2: demonstrated failure to converge with overlapping clusters.  </li> <li>Visualizations: complete set of scatter plots, decision boundaries, accuracy curves.  </li> <li>Mathematical explanation: update rule, convergence theorem, and theoretical expectations included.  </li> <li>Discussion: results analyzed in depth and compared with theoretical expectations.  </li> </ul>"},{"location":"exercises/vae/main/","title":"VAE Task","text":""},{"location":"exercises/vae/main/#variational-autoencoder-on-mnist","title":"Variational Autoencoder on MNIST","text":"<p>The notebook is available in the repository inside the <code>notebooks</code> folder.</p>"},{"location":"exercises/vae/main/#1-introduction","title":"1. Introduction","text":"<p>The goal of this project is to implement, train, and analyze a Variational Autoencoder (VAE) using the MNIST dataset. A VAE is a generative model capable of learning a continuous latent representation of the data and generating new samples by decoding latent vectors.</p> <p>This report covers:</p> <ol> <li>Dataset and preprocessing  </li> <li>VAE architecture (encoder, decoder, reparameterization)  </li> <li>Loss function and training procedure  </li> <li>Reconstruction of images  </li> <li>Latent space visualization  </li> <li>Generation of new samples  </li> <li>Training loss analysis  </li> <li>Discussion and conclusion  </li> </ol> <p>All experiments were implemented in PyTorch.</p>"},{"location":"exercises/vae/main/#2-dataset","title":"2. Dataset","text":""},{"location":"exercises/vae/main/#21-description","title":"2.1. Description","text":"<p>The project uses the MNIST dataset, which contains:</p> <ul> <li>60,000 training images  </li> <li>10,000 test images  </li> <li>Grayscale handwritten digits (0\u20139)  </li> <li>Resolution: 28 \u00d7 28 pixels</li> </ul> <p>MNIST is ideal for evaluating generative models due to its simplicity and structure.</p>"},{"location":"exercises/vae/main/#22-preprocessing","title":"2.2. Preprocessing","text":"<p>To ensure compatibility with Binary Cross-Entropy (BCE) reconstruction loss, images were only converted to tensors:</p> <pre><code>transform = transforms.ToTensor()\n````\n\nThis keeps pixel intensities in the range **[0, 1]**, as required by BCE.\n\nThe dataset was loaded using:\n\n```python\ndatasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\ndatasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n</code></pre> <p>Batch size was set to 128.</p>"},{"location":"exercises/vae/main/#3-vae-architecture","title":"3. VAE Architecture","text":"<p>A Variational Autoencoder consists of:</p> <ol> <li>Encoder \u2192 maps input image to latent distribution parameters (\u03bc and log \u03c3\u00b2)</li> <li>Reparameterization Trick \u2192 samples latent vector z in a differentiable way</li> <li>Decoder \u2192 reconstructs images from latent vectors</li> </ol> <p>The latent dimension was intentionally set to 2, enabling direct visualization.</p>"},{"location":"exercises/vae/main/#31-encoder","title":"3.1. Encoder","text":"<p>The encoder processes a flattened 784-dimensional vector (28\u00d728) using:</p> <pre><code>Linear(784 \u2192 400) \u2192 ReLU\n</code></pre> <p>From this representation, two linear layers output:</p> <ul> <li>\u03bc (mu)</li> <li>log \u03c3\u00b2 (logvar)</li> </ul> <p>These define the approximate posterior distribution:</p> <p>\\([q(z|x) = N(\\mu, \\sigma^2)]\\)</p>"},{"location":"exercises/vae/main/#32-reparameterization-trick","title":"3.2. Reparameterization Trick","text":"<p>To allow gradient flow through the sampling operation, we rewrite:</p> <p>\\([ z = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim N(0, I) ]\\)</p> <p>Implemented as:</p> <pre><code>std = torch.exp(0.5 * logvar)\neps = torch.randn_like(std)\nz = mu + eps * std\n</code></pre>"},{"location":"exercises/vae/main/#33-decoder","title":"3.3. Decoder","text":"<p>The decoder reconstructs the image using:</p> <pre><code>Linear(latent_dim \u2192 400) \u2192 ReLU \u2192 Linear(400 \u2192 784) \u2192 Sigmoid\n</code></pre> <p>Sigmoid ensures the output is in [0,1], matching the input range required for BCE.</p>"},{"location":"exercises/vae/main/#4-loss-function","title":"4. Loss Function","text":"<p>The VAE loss combines:</p> <ol> <li>Reconstruction Loss (BCE)</li> <li>KL Divergence between the approximate posterior and the unit Gaussian prior.</li> </ol>"},{"location":"exercises/vae/main/#41-reconstruction-loss","title":"4.1. Reconstruction Loss","text":"<p>\\([ \\text{BCE}(x, \\hat{x}) = -\\sum_i [x_i \\log \\hat{x}_i + (1-x_i)\\log(1-\\hat{x}_i)] ]\\)</p>"},{"location":"exercises/vae/main/#42-kl-divergence","title":"4.2. KL Divergence","text":"<p>\\([ \\text{KL}(q(z|x) || p(z)) = -\\frac{1}{2} \\sum_j (1 + \\log\\sigma_j^2 - \\mu_j^2 - \\sigma_j^2) ]\\)</p>"},{"location":"exercises/vae/main/#43-total-loss","title":"4.3. Total Loss","text":"<p>\\([ \\mathcal{L} = \\text{BCE} + \\text{KLD} ]\\)</p>"},{"location":"exercises/vae/main/#both-terms-work-together-bce-ensures-accurate-reconstruction-kl-regularizes-the-latent-space","title":"Both terms work together: BCE ensures accurate reconstruction; KL regularizes the latent space.","text":""},{"location":"exercises/vae/main/#5-training-procedure","title":"5. Training Procedure","text":""},{"location":"exercises/vae/main/#51-optimizer-hyperparameters","title":"5.1. Optimizer &amp; Hyperparameters","text":"<ul> <li>Optimizer: Adam</li> <li>Learning rate: 1e-3</li> <li>Epochs: 20</li> <li>Batch size: 128</li> <li>Device: CUDA when available</li> </ul>"},{"location":"exercises/vae/main/#52-loop-summary","title":"5.2. Loop Summary","text":"<p>Each training iteration:</p> <ol> <li>Forward pass: compute <code>x_hat</code>, <code>mu</code>, <code>logvar</code></li> <li>Compute loss</li> <li>Backpropagate gradients</li> <li>Update parameters</li> <li>Track epoch loss</li> </ol>"},{"location":"exercises/vae/main/#53-observed-behavior","title":"5.3. Observed Behavior","text":"<ul> <li>Steady decline in loss over epochs</li> <li>Convergence reached after ~15 epochs</li> <li>Smooth behavior indicates stable training</li> </ul>"},{"location":"exercises/vae/main/#54-training-loss-curve","title":"5.4. Training Loss Curve","text":"<p>The following plot shows the evolution of the total training loss over the 20 epochs:</p> <p></p>"},{"location":"exercises/vae/main/#analysis","title":"Analysis","text":"<ul> <li>The loss decreases monotonically, confirming successful optimization.</li> <li>The curve stabilizes near the end, indicating convergence.</li> <li>Slight noise is expected due to the stochastic KL component.</li> </ul> <p>Monitoring this curve is essential for verifying that the VAE is learning properly.</p>"},{"location":"exercises/vae/main/#6-results","title":"6. Results","text":""},{"location":"exercises/vae/main/#61-reconstructed-images","title":"6.1. Reconstructed Images","text":"<p>The VAE was evaluated on test images. The figure below compares:</p> <ul> <li>Top row: original inputs</li> <li>Bottom row: reconstructions</li> </ul> <p></p>"},{"location":"exercises/vae/main/#interpretation","title":"Interpretation","text":"<ul> <li>The model captures digit shapes well.</li> <li>Some details are smoothed due to the small latent dimension (2).</li> <li>Reconstructions are coherent and readable.</li> </ul>"},{"location":"exercises/vae/main/#62-latent-space-visualization","title":"6.2. Latent Space Visualization","text":"<p>With a 2D latent space, each test image can be mapped to a point (z\u2081, z\u2082):</p> <p></p>"},{"location":"exercises/vae/main/#interpretation_1","title":"Interpretation","text":"<ul> <li>Digits form distinct clusters, showing class separation.</li> <li>Similar digits (e.g., 3 and 8) occupy nearby regions.</li> <li>The continuous manifold implies smooth interpolation is possible.</li> </ul> <p>This demonstrates that the VAE learned a meaningful structure of the data distribution.</p>"},{"location":"exercises/vae/main/#63-generated-samples","title":"6.3. Generated Samples","text":"<p>Random points were sampled from the prior distribution:</p> <p>\\([ z \\sim N(0, I) ]\\)</p> <p>Decoded images are shown below:</p> <p></p>"},{"location":"exercises/vae/main/#interpretation_2","title":"Interpretation","text":"<ul> <li>Many samples resemble valid MNIST digits.</li> <li>Some are ambiguous\u2014expected for a simple fully connected VAE.</li> <li>The model successfully generates new data, not seen during training.</li> </ul>"},{"location":"exercises/vae/main/#7-discussion","title":"7. Discussion","text":""},{"location":"exercises/vae/main/#71-latent-representation-quality","title":"7.1. Latent Representation Quality","text":"<p>Using <code>latent_dim = 2</code> makes the learned structure easy to interpret. The latent clusters reveal:</p> <ul> <li>How the model organizes visual patterns</li> <li>Smooth transitions between digits</li> <li>Overlap between visually similar classes</li> </ul>"},{"location":"exercises/vae/main/#72-reconstruction-vs-regularization-trade-off","title":"7.2. Reconstruction vs. Regularization Trade-off","text":"<p>VAEs naturally balance:</p> <ul> <li>High reconstruction accuracy</li> <li>A smooth, well-behaved latent space</li> </ul> <p>Increasing reconstruction quality typically increases KL divergence, and vice versa.</p>"},{"location":"exercises/vae/main/#73-limitations","title":"7.3. Limitations","text":"<ul> <li>Latent space limited to 2 dimensions</li> <li>Fully-connected architecture lacks spatial modeling</li> <li>Reconstructions are blurrier than a convolutional VAE</li> </ul>"},{"location":"exercises/vae/main/#74-potential-improvements","title":"7.4. Potential Improvements","text":"<ul> <li>Replace dense layers with CNNs</li> <li>Increase latent dimension</li> <li>Experiment with \u03b2-VAE for disentanglement</li> <li>Train on Fashion-MNIST for more complex shapes</li> </ul>"},{"location":"exercises/vae/main/#8-conclusion","title":"8. Conclusion","text":"<p>This project successfully implemented a Variational Autoencoder for the MNIST dataset and demonstrated:</p> <ul> <li>Effective training and convergence</li> <li>Quality image reconstructions</li> <li>A well-structured 2D latent space</li> <li>Ability to generate novel, realistic digits</li> </ul> <p>The results confirm that VAEs are powerful tools for both representation learning and generative modeling. Even with a simple architecture and a very low-dimensional bottleneck, the model learns meaningful latent structure and produces convincing outputs.</p>"},{"location":"generative-project/main/","title":"Generative Project","text":""},{"location":"generative-project/main/#generative-project","title":"Generative Project","text":"<p>Documentation &amp; Report: https://thomaschiari.github.io/deep-learning-coursework/generative-project/generative-project/</p>"},{"location":"regression-project/main/","title":"Regression Project","text":""},{"location":"regression-project/main/#regression-project","title":"Regression Project","text":"<p>Documentation &amp; Report: https://thomaschiari.github.io/deep-learning-coursework/regression-project/regression-project/</p>"},{"location":"thisdocumentation/main/","title":"This documentation","text":""},{"location":"thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}